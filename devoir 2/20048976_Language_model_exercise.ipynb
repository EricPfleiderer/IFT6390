{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20048976_Language model exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flax optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyqbtmU30Q5U",
        "outputId": "cee25133-3f22-4524-99a7-c65aafd2cd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flax\n",
            "  Downloading flax-0.4.1-py3-none-any.whl (184 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 61 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 81 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 102 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 112 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 122 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 133 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 143 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 153 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 163 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 174 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184 kB 4.6 MB/s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import optax\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "Sa_ddfY2SEkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to learn a [basic language model](https://en.wikipedia.org/wiki/Language_model) using an recurrent neural network. \n",
        "\n",
        "As as starting point, you should implement a simple RNN of the form (for reference, see the Deep Learning textbook chapter 10 page 370): \n",
        "\\begin{align*}\n",
        "a^{(t)} &= Wh^{(t-1)} + Ux^{(t)} + b \\\\\n",
        "h^{(t)} &= \\tanh(a^{(t)})\\\\\n",
        "o^{(t)} &= Vh^{(t)} + c \\enspace ,\n",
        "\\end{align*}\n",
        "where $h^{(t)}$ is the updated state at time $t$ , $x^{(t)}$ is the input and $o^{(t)}$ is the output. Given an initial input $x^{(0)}$ and hidden state $h^{(0)}$, an RNN computes the output sequence $o^{(0)}, ..., o^{(T)}$ by applying $f$ recursively: \n",
        "\\begin{align*}\n",
        "(h^{(t+1)}, o^{(t+1)}) = f(h^{(t)}, x^{(t)}; \\theta) \\enspace .\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "RAWCIVnxFLdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ElmanCell(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, state, x):\n",
        "    # IMPLEMENT the basic RNN cell described above (outputting h^{t} only)\n",
        "    x = jnp.concatenate([state[0], x])\n",
        "    a = jnp.tanh(nn.Dense(state[0].shape[0])(x))\n",
        "    return jnp.tanh(a)"
      ],
      "metadata": {
        "id": "i6xzFD-DSKrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutiple such cells can be chained up together to attain more expressivity, for example, we can link two cells as follows:\n",
        "\\begin{align*}\n",
        "h_1^{(t+1)} &= \\tanh(W_1h_1^{(t)} + U_1x^{(t)} + b_1)\\\\\n",
        "h_2^{(t+1)} &= \\tanh(W_1h_1^{(t)} + U_1h_1^{(t)} + b_1)\\\\\n",
        "o^{(t+1)} &= Vh_2^{(t+1)} + c \\enspace ,\n",
        "\\end{align*}\n",
        "and the resulting network is of the form: \n",
        "\\begin{align*}\n",
        "(h_1^{(t+1)}, h_2^{(t+1)}, o^{(t+1)}) = f(h_1^{(t)}, h_2^{(t)}, x^{(t)}; \\theta) \\enspace .\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "HzgGOmfNHHKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentNetwork(nn.Module):\n",
        "  state_size: int\n",
        "  num_classes: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, state, i):\n",
        "    x = jnp.squeeze(jax.nn.one_hot(i, self.num_classes))\n",
        "    # IMPLEMENT\n",
        "    # Build a simple RNN with a single cell\n",
        "    state = ElmanCell()(state, x)\n",
        "    predictions = nn.Dense(features=state.shape[0])(state)\n",
        "    return (state,), predictions\n",
        "\n",
        "  def init_state(self):\n",
        "    return (jnp.zeros(self.state_size),)"
      ],
      "metadata": {
        "id": "Y0dqECaIHH10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learn our language model by taking a written document and learn to predict the next character (a character-level language model) using our RNN. This is prediction task is akin to a classification and we therefore use the cross-entropy loss: \n",
        "\\begin{align*}\n",
        "l(x, y, \\theta) \\triangleq -\\log p_\\theta(y|x) \\enspace.\n",
        "\\end{align*}\n",
        "We compute those probabilities using our RNN. The output $o^{(t)}$ represent the the so-called **logits** which can be transformed into probabilities using the softmax function. That is $\\text{softmax}(o^{(t)})$ gives us the desired probabilities. "
      ],
      "metadata": {
        "id": "PEU-BgnOJj_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_rnn_loss(model):\n",
        "  def cross_entropy(logits, target):\n",
        "    \"\"\" Negative cross-entropy\n",
        "    Args:\n",
        "      logits: jnp.ndarray with num_classes dimensions (unbatched, batching is done outside through vmap)\n",
        "      target (int): target class that should have been predicted\n",
        "    \"\"\"\n",
        "    # IMPLEMENT the cross-entropy loss\n",
        "    # Hint: use jax.nn.log_softmax to avoid computing the log and \n",
        "    # the softmax separately. This function is numerically more stable that the \n",
        "    # naive approach. \n",
        "    probs = jax.nn.log_softmax(logits)\n",
        "    return -jnp.sum(target * probs)\n",
        "    \n",
        "  def rnn_loss(params, inputs, targets, init_state):\n",
        "    final_state, logits = jax.lax.scan(partial(model.apply, params), init_state, inputs)\n",
        "    loss = jnp.mean(jax.vmap(cross_entropy, in_axes=(0, 0))(logits, targets.astype(jnp.int32)))\n",
        "    return loss, final_state\n",
        "  return jax.jit(rnn_loss)"
      ],
      "metadata": {
        "id": "2EOF-OOBSp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the above function, we unroll the ``jax.lax.scan`` over a given input sequence and compute the loss along. When it comes to generating new content, we have to execute our RNN differently. That is: we provide a starting hidden state and character $x^{(0)}$, compute the distribution over next character using the softmax transformation of the logits computed as output of the RNN, sample one of those next character, and repeat the process in this manner until we reach a desired length. In other words, we generate a string *auto-regressively*. \n",
        "\n",
        "A variant on the above procedure is to let the RNN start from more than just a given single character and instead pass a longer *prompt*. The same idea holds except that we have to ``jax.lax.scan`` over as many characters as we have in our prompt. \n",
        "\n",
        "The process described above is *stochastic* in nature: the next character is sampled according to the predicted class distribution. When using the softmax transformaiton,  can vary the degree of stochasticity using a temperature parameter $\\tau$. All we have to do is to multiply the logits by the inverse temperature: $\\text{softmax}((1/\\tau)o^{(t)})$. The smaller the temperature, the more deterministic the model becomes. \n"
      ],
      "metadata": {
        "id": "GblzQ6icMKJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(key, model, params, id_lookup, chr_lookup, prompt='', max_length=100, temperature=1.):  \n",
        "  encoded_prompt = jnp.asarray(list(map(lambda c: id_lookup[c], prompt)))\n",
        "  state, _ = jax.lax.scan(partial(model.apply, params), model.init_state(), encoded_prompt[:-1])\n",
        "\n",
        "  num_classes = len(id_lookup)\n",
        "  def autoregressive_step(carry, subkey):\n",
        "    state, last_char = carry\n",
        "    state, logits = model.apply(params, state, last_char)\n",
        "    \n",
        "    # IMPLEMENT\n",
        "    probs = jax.nn.log_softmax((1/temperature)*logits)\n",
        "    choice = jnp.random.choice(probs) # sample the next character at the given temperature\n",
        "    prediction = jnp.where(probs==choice)\n",
        "    # prediction = jnp.argmax(probs)\n",
        "    \n",
        "    return (state, prediction), prediction\n",
        "  keys = jax.random.split(key, max_length)\n",
        "  _, sequence = jax.lax.scan(autoregressive_step, (state, id_lookup[prompt[-1]]), xs=keys)\n",
        "  decoded_sequence = list(map(lambda i: chr_lookup[int(i)], sequence))\n",
        "\n",
        "  return prompt + ''.join(decoded_sequence)"
      ],
      "metadata": {
        "id": "ERyDwz3ISids"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code doesn't have to be modified. Its purpose is to turn the text into one-hot vectors (features) and to chunk up the text (which can be very large) into smaller and more manageable subsequences. "
      ],
      "metadata": {
        "id": "U7SfuqpeOEYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(x, seq_size):\n",
        "  if seq_size > x.shape[0]:\n",
        "    return jnp.atleast_2d(x[:-1]), jnp.atleast_2d(x[1:])\n",
        "  num_partitions = int(jnp.ceil(x.shape[0]/seq_size))\n",
        "  inputs = jnp.array_split(x, num_partitions)\n",
        "  targets = jnp.array_split(jnp.append(x[1:], jnp.nan), num_partitions)\n",
        "  return inputs[:x.shape[0] % num_partitions], targets[:x.shape[0] % num_partitions]"
      ],
      "metadata": {
        "id": "oLBRvWeglJuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_subsequence(key, data, size):\n",
        "    ridx = jax.random.randint(key, (1,), minval=0, maxval=data.shape[0]-size)[0]\n",
        "    return data[ridx:ridx+size]"
      ],
      "metadata": {
        "id": "1irXNSaZ1_z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  unique_chars = set(data)\n",
        "  id_lookup = dict(zip(unique_chars, range(len(unique_chars))))\n",
        "  chr_lookup = dict(zip(range(len(unique_chars)), unique_chars))\n",
        "  encoded_data = jnp.asarray(list(map(lambda c: id_lookup[c], data)))\n",
        "  return encoded_data, id_lookup, chr_lookup, len(unique_chars)"
      ],
      "metadata": {
        "id": "Au_jKTsk28br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the large size of the training sequence (the entire text document), we have to split it into manageable subsequences. More precisely, at every training *epoch* we sample a contiguous subsequence from the entire document and compute the negative log likelihood loss by unrolling the RNN over the given characters. However, given the challenge (more on this in question) of learning over long horizon, we truncate the unroll over fewer characters and warm start the initial state between each such truncated unroll. "
      ],
      "metadata": {
        "id": "7264m-Bs8_UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(key, data, state_size, learning_rate, n_epochs, batch_size, max_subsequence_length, sample_length, test_prompt=None, temperature=1.):\n",
        "  encoded_data, id_lookup, chr_lookup, num_classes = preprocess(data)\n",
        "  model = RecurrentNetwork(state_size, num_classes)\n",
        "  params = model.init(key, model.init_state(), 0)\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=learning_rate)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  rnn_loss_grad = jax.value_and_grad(make_rnn_loss(model), has_aux=True)\n",
        "  opt_state = optimizer.init(params)\n",
        "  for i in range(n_epochs):\n",
        "      key, subkey = jax.random.split(key)\n",
        "      subsequence = sample_subsequence(key, encoded_data, max_subsequence_length).astype(jnp.int32)\n",
        "\n",
        "      state = model.init_state()\n",
        "      batch_losses = []\n",
        "      for inputs, targets in zip(*chunk(subsequence, batch_size)):\n",
        "        (loss, state), gradient = rnn_loss_grad(params, inputs, targets, state)\n",
        "        updates, opt_state = optimizer.update(gradient, opt_state)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        batch_losses.append(loss)\n",
        "      if not (i % 10):\n",
        "        if test_prompt is None:\n",
        "          test_prompt = data[:4]\n",
        "        generated_string = sample(key, model, params, id_lookup, chr_lookup, test_prompt, max_length=sample_length, temperature=temperature)\n",
        "        print(f\"Epoch {i} Average loss: {jnp.mean(jnp.asarray(batch_losses)):.5f} random sample: {generated_string}\") "
      ],
      "metadata": {
        "id": "k8NljkfVcm5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "We learn our language model over a children book called \"The Life and Adventures of Santa Claus\" by L. Frank Baum. "
      ],
      "metadata": {
        "id": "lnx2IpAN-kFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gutenberg.org/cache/epub/520/pg520.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS_17yTck7eX",
        "outputId": "c8201e89-c279-4cc2-e322-b6764c44a4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-12 02:51:32--  https://gutenberg.org/cache/epub/520/pg520.txt\n",
            "Resolving gutenberg.org (gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to gutenberg.org (gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 188277 (184K) [text/plain]\n",
            "Saving to: ‘pg520.txt’\n",
            "\n",
            "pg520.txt           100%[===================>] 183.86K   678KB/s    in 0.3s    \n",
            "\n",
            "2022-03-12 02:51:33 (678 KB/s) - ‘pg520.txt’ saved [188277/188277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pg520.txt', 'r') as file:\n",
        "  data = file.read()"
      ],
      "metadata": {
        "id": "Z-rbHYnsA0NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training your model, you should be able to observe that the samples become more coherent over time while the log likelihood loss goes down. "
      ],
      "metadata": {
        "id": "A7TzNdYU--EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "train(key, data, state_size=256, learning_rate=1e-3, batch_size=64, n_epochs=2000, max_subsequence_length=5000, sample_length=50, test_prompt='Santa ', temperature=1e-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t2iLRP-CA3Y",
        "outputId": "76de72c1-4117-4a89-a955-44c98ec4aaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Average loss: 3.43944 random sample: Santa                                                   \n",
            "Epoch 10 Average loss: 2.82208 random sample: Santa  ane the the the the the the the the the the the t\n",
            "Epoch 20 Average loss: 2.36441 random sample: Santa the he the sand he the sand he the sand he the san\n",
            "Epoch 30 Average loss: 2.41000 random sample: Santa the the the the the the the the the the the the th\n",
            "Epoch 40 Average loss: 2.30507 random sample: Santa the tored the sered the sored the seres wore the t\n",
            "Epoch 50 Average loss: 2.28557 random sample: Santa dout the Fore the the seren the seer and he wher a\n",
            "Epoch 60 Average loss: 2.15486 random sample: Santa the sored the he the the deat the he the could the\n",
            "Epoch 70 Average loss: 2.10079 random sample: Santa Claus the shinged the sous the seas the she sead t\n",
            "Epoch 80 Average loss: 2.41078 random sample: Santa Claus or the cored torker of the cored torker of t\n",
            "Epoch 90 Average loss: 2.18038 random sample: Santa Claus he he he he he he he he he he he he he he he\n",
            "Epoch 100 Average loss: 2.88861 random sample: Santa Claus of coment Gutent on and the Project Gutent o\n",
            "Epoch 110 Average loss: 2.12971 random sample: Santa Claus the laden the sto the child ghe the ghe the \n",
            "Epoch 120 Average loss: 2.11627 random sample: Santa Claus the calle the calle the calle the calle the \n",
            "Epoch 130 Average loss: 2.17712 random sample: Santa Claus the comed the berent he prepling the bere th\n",
            "Epoch 140 Average loss: 2.11828 random sample: Santa Claus for sto the callered the caller him the sing\n",
            "Epoch 150 Average loss: 2.00210 random sample: Santa Claus for the colle the colly and ald and sere the\n",
            "Epoch 160 Average loss: 2.00832 random sample: Santa Claus ware the wored the wood the wored to the war\n",
            "Epoch 170 Average loss: 2.01791 random sample: Santa Claus wathe has ween the King the King the King th\n",
            "Epoch 180 Average loss: 2.00310 random sample: Santa Claus work the Food the Food the Food the Food the\n",
            "Epoch 190 Average loss: 1.96233 random sample: Santa Claus the came the came the rears and the pare the\n",
            "Epoch 200 Average loss: 1.92799 random sample: Santa Claus found the wall the wall the wall the wook in\n",
            "Epoch 210 Average loss: 1.94283 random sample: Santa Claus for he wall and the rowere the roushed the r\n",
            "Epoch 220 Average loss: 1.92229 random sample: Santa Claus to the been the sough beaghed to the sough a\n",
            "Epoch 230 Average loss: 1.85965 random sample: Santa Claus the beat he with the sond the cond the conde\n",
            "Epoch 240 Average loss: 1.93476 random sample: Santa Claus ond ham he has the Knotken the King and the \n",
            "Epoch 250 Average loss: 1.90188 random sample: Santa Claus for the wored the wored the wored the wored \n"
          ]
        }
      ]
    }
  ]
}